```{r, include=FALSE, cache=FALSE}
source("~/dev/projects/Chen_environmental_study/reports/report_jackknife/Jackknife_variance.R")
```

## Jackknife Vairance 
$S(X_1, \dots, X_n)$ is a statistic of interest, define 
\[
S_{(i)} = S(X_1, X_{i-1}, X_{i+1}\dots, X_n)
\]
as the delete-1 result of $S$.
If we delete each observation, then we will get n $S_{(i)}$. We could use those n subsample to estimate the variance of $S$ on original n dataset as following, 
\[
\widehat{VAR}~S(X_1, \dots, X_n) = \frac{n-1}{n}\sum_{i}^n(S_{(i)} -S_{(.)} )^2
\],
where $S_{(.)} = \frac{\sum_{i}^nS_{(i)}}{n}$.
The variance estimation actually can be considered into a two-step process

1. Estimate the variance of $S$ with n-1 sample:
\[
\widehat{VAR}~S(X_1, X_{i-1}, X_{i+1}\dots, X_n) := \widetilde{VAR}~S(X_1, X_{i-1}, X_{i+1}\dots, X_n) = \sum_i^{n}(S_{(i)} -S_{(.)} )^2,
\]
which could be considered as an modification of the variance estimation corresponding to the dependency of the n delete-1 subsamples. That is originally we need a coefficient $\frac{1}{n-1}$ for sample variance if the samples are indepedent. But the delete-1 subsamples are high dependent to each other, so intuitively the sample variance will underestimate the variance. In order to alleviate the underestimation, it seems that we multiply  $n-1$. 
\[
n-1 \cdot \frac{1}{n-1} \cdot \sum_i^{n}(S_{(i)} -S_{(.)} )^2 = \sum_i^{n}(S_{(i)} -S_{(.)} )^2.
\]
However, by doing this, the result become overestimated and that will be discussed in the following sections.

1. Modification the variance of $n-1$ samples to $n$ samples by:
\[
\widehat{VAR}~S(X_1, \dots, X_n) = \frac{n-1}{n} \widetilde{VAR}~S(X_1, X_{i-1}, X_{i+1}\dots, X_n).
\]

## Bias of Variance estimation 

In the Efron 1981's paper, it shows that 
\[
E\left[\widetilde{VAR}~S(X_1, X_{i-1}, X_{i+1}\dots, X_n)\right] \geq VAR~S(X_1, X_{i-1}, X_{i+1}\dots, X_n).
\]
The details of proof could be found by [@efron1981], the idea of the proof is the ANOVA decomposition:
\[
\begin{aligned} 
S\left(X_{1}, X_{2}, \cdots, X_{n}\right)=\mu 
&+\sum_{i} A_{i}\left(X_{i}\right)+\sum_{i<i'} B_{i '}\left(X_{i}, X_{i'}\right) \\ 
&+\sum_{i<i'<i''}C_{ii'i''}\left(X_{i}, X_{i'}, X_{i''}\right)+\cdots+H\left(X_{1}, X_{2}, \cdots, X_{n}\right) 
\end{aligned}
\],
where $\mu = E(S)$,  $A_{i}\left(x_{i}\right)=E\left\{S | X_{i}=x_{i}\right\}-\mu$ and $B_{i t}\left(x_{i}, x_{i}\right)=E\left(S | X_{i}=x_{i}, X_{i}=x_{i t}\right\}-E\left\{S | X_{i}=x_{i}\right)-E\left\{S | X_{i}=x_{i}\right\}+\mu$. $A$ is the analogy of main effect and $B$ is for the two-term interaction effects. Note that after the ANOVA decompostion, all the terms has mean **0** and correlation **0**. Therefore we have 
\[
\begin{aligned} S\left(X_{1}, X_{2}, \cdots, X_{n}\right)=& \mu+\frac{1}{n} \sum_{i} \alpha_{i}+\frac{1}{n^{2}} \sum_{i<i'} \beta_{i i} \\ &+\frac{1}{n^{3}} \sum_{i<i<i''} \gamma_{i i' i''}+\cdots+\frac{1}{n^{n}} \eta_{1,2,3, \ldots, n} \end{aligned},
\]
where $\begin{array}{l}{\alpha_{i} \equiv \alpha\left(X_{i}\right) \equiv n A\left(X_{i}\right), \quad \beta_{i i}=\beta\left(X_{i}, X_{i}\right) \equiv n^{2} B\left(X_{i}, X_{i^{\prime}}\right)} \\ {\gamma_{u i^{*}}=\gamma\left(X_{i}, X_{i^{\prime}}, X_{i^{*}}\right)=n^{3} C\left(X_{i}, X_{i}, X_{i^{*}}\right), \cdots}\end{array}$. 
Then since all of them are uncorrelated, we could take the variance on both side and have 
\[
\operatorname{Var} S\left(X_{1}, X_{2}, \cdots, X_{n}\right)=\frac{\sigma_{a}^{2}}{n}+\left(\begin{array}{c}{n-1} \\ {1}\end{array}\right) \frac{\sigma_{\beta}^{2}}{2 n^{3}}+\left(\begin{array}{c}{n-1} \\ {2}\end{array}\right) \frac{\sigma_{\gamma}^{2}}{3 n^{5}}+\cdots+\frac{\sigma_{n}^{2}}{n^{2 n}}.
\]
It can also shown that 
\[
\begin{aligned} E\left(\widetilde{\operatorname{VAR}} S\left(X_{1}, X_{2}, \cdots, X_{n-1}\right)\right)=& \frac{\sigma_{\alpha}^{2}}{n-1} \\ &+\left(\begin{array}{c}{n-2} \\ {1}\end{array}\right) \frac{\sigma_{\beta}^{2}}{(n-1)^{2}}+\left(\begin{array}{c}{n-2} \\ {2}\end{array}\right) \frac{\sigma_{r}^{2}}{(n-1)^{3}}+\cdots \end{aligned}
\],
so we have 
\[
\begin{aligned} E\left(\widetilde{\operatorname{VAR}} S\left(X_{1}, X_{2}, \cdots, X_{n-1}\right)\right\}-\operatorname{Var} S\left(X_{1}, X_{2}, \ldots, X_{n-1}\right) \\=\frac{1}{2}\left(\begin{array}{c}{n-2} \\ {1}\end{array}\right) \frac{\sigma_{N}^{2}}{(n-1)^{3}}+\frac{2}{3}\left(\begin{array}{c}{n-2} \\ {2}\end{array}\right) \frac{\sigma_{r}^{2}}{(n-1)^{s}}+\cdots \end{aligned}
\].
Note that bias of the variance comes from the variance of high order interactions. If $S$ is a **linear** functional the emprical cumulative density function, the the bias is 0. However, if it is not, then there will be a non-zero bias. Although Efron suggested a bias correction method, but it is not very practical which I will mention in the next section.  

For certain types of $S$, the bias of the variance will be reduced by increasing of n. 
\[
E\hat{Var} = Var^{(n)} + \{\frac{n-1}{n}Var^{(n-1)} - Var^{(n)})\} + O(1/n^3),
\]

### Functionals of emprical distribution function

## Bias correction 

### Using delete-1-2 method 
If we assume the $S$ is a smooth functions of emperical CDF, especially a **quadratic** functions, then it can be shown the leading terms of $E(\tilde{Var}(S(X_1, \dots, S_{n-1}))) \geq Var(S(X_1, \dots, S_{n-1}))$ is a quadratic term in expecation. Therefore we could try to estimate the quadratic term and correct the bias for the jackknife variance estimation.

Define $Q_{ii'} \equiv nS - (n-1)(S_{i} + S_{i'}) + (n-2)S_{(ii')}$, then the correction will be 
\[
\hat{Var}^{corr}(S(X_1, \dots, X_n)) = \hat{Var}(S(X_1, \dots, X_n)) - \frac{1}{n(n-1)}\sum_{i < i'}(Q_{ii'}- \bar{Q})^2
\]
where $\bar{Q} = \sum_{i < i'}(Q_{ii'})/(n(n-1)/2)$

1. One potential issue of this method is that it cannot guarantee the corrected variance is positive. In other words, some times the bias correction is overestimating the bias so that ending a negative variance. This issue is not unexpected, because the correction is based only on the quadratic form.
2. Another issue is the computational time. To calculate the variance correction, one needs to do $n \choose 2$ times iteration, which will be time comsuming for large n.

### Delete-d method
The delete-d jackknife method is porposed In [@shao1989general], The delete-d jackknife varinace estimator is 
\[
\mathcal{V}_{J(d)} = \frac{n-d}{d} \cdot \frac{1}{N}\sum_{S}(\hat{\theta}_S - \hat{\theta}_{S.} )
\],
where $N =\binom{n}{d}$ and $S$ is subset of $x_1, \dots, x_n$ with size $n - d$.
Note that delete-1 jackknife will be a special case of delete-d case variance estimation:
\[
\mathcal{V}_{J(1)} = \frac{n-1}{1} \cdot \frac{1}{N}\sum_{S}(\hat{\theta}_S - \hat{\theta}_{S.} )
\]
where $N =\binom{n}{1} = n$. But how could we explain the 2-steps estimation in Eforn's 1989 paper?

Note that S could a very large value, so in the following simulation, only $S = 1000$ is used. In Jun Shao's another paper, he proposed an approximation of the deletel-d variance estimation. That is just select m from $S =\binom{n}{d}$ sub-samples and in that paper it recommended $m = n^{1.5}$.

#### An example of delete-d and delete-1: median
$S_n = F^{-1}_n(1/2)$
The simulation setup is following

```{r, echo=F}
data <- rbind(summary_result_median_jack_d_1,
              summary_result_median_jack_d_d,
              fill = TRUE) %>% setorder(., d,n)

kable(data, "latex",  booktabs = T, linesep = "") %>%
kable_styling(latex_options = c("repeat_header", "scale_down")) 
```




