---
title: "SVD Dimension reduction method"
author: "Xuelong Wang"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: true
    number_sections: true
    keep_tex: true
    fig_width: 12
    fig_height: 4
header-includes:
    - \usepackage{float,amsmath, bbm, siunitx, bm}
    - \usepackage{pdfpages}
    - \floatplacement{figure}{H}
    - \newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = FALSE, message = FALSE, warning = FALSE, fig.height = 10)
knitr::opts_chunk$set(fig.pos = 'h')

library(MASS)
library(tidyverse)
source("../../R_code/Yang_REML.R")
```

# Motivation

Based on previous simulation results we did a series of simulation on estimation of total variance of main and interactive effects. we found that combing dimension reduction with decorrelation tend (our proposed method) to have a better result than GCTA, especailly when n < p and correlation between covariates are high. Therefore, we condcuted a group of simulation studies trying to evaluate the performance of the proposed method. we tried different covariance structures and PCBs data with re-sampling. Overall, the performance is good in most of the case. When n is small and correlation is also weak, the prospoed method is as good as the original GCTA method. 



# Main idea two steps 

## Dimension Reduction

\begin{align*}
  X = U D V^T &= \begin{bmatrix}
                      U_r & U_2
                      \end{bmatrix}
                      \begin{bmatrix}
                      D_r & 0\\
                      0 & D_2
                      \end{bmatrix}
                      \begin{bmatrix}
                      V_r & V_2\\
                      V_3 & V_4
                      \end{bmatrix}^T \\ 
              &= 
                      \begin{bmatrix}
                      U_rD_r & U_2D_2
                      \end{bmatrix}
                      \begin{bmatrix}
                      V_r^T & V_3^T\\
                      V_2^T & V_4^T
                      \end{bmatrix}
                      =
                      \begin{bmatrix}
                      U_rD_rV_r^T + U_2D_2V_2^T & U_rD_rV_3^T + U_2D_2 V_4^T
                      \end{bmatrix}
\end{align*}

Ignore $V_2$, $V_3$ and $V_4$ , then we have the X_r as following
\[
  X_r = U_rD_rV_r^T.
\]
We use $X_r$ as the new covariates to the proposed methd. Therefore, we reduce the dimension from p to n

## Following with GCTA method

After calculating $X_r$, we can regard $X_r$ as our new predictors and use it as the input to the proposed method

Note that we could use this blocking method to reduce X's dimension to $k, k \leq min(p,n)$. 

# Simulation study 

I used Chi-square random variable with df = 1. To generate a certain covariance structure, one could randomly generate a sample from multivariate-normal-distribution first, and then just square each elements to have a group univarate Chi-saure distribution with desired correlations. The details of simulation is shown as follows.

## Simulation setup 

1. Normal distribution  
  \[
    X = [X_1 \dots, X_p] ~~~ cov(X_i, X_j) = \Sigma_{X}
  \]
  
1. Chi-square distribution  
  \[
    T = [T_1 \dots, T_p] ,~~~ T_i = X_i^2 \sim \chi_{(1)}^2, ~~~ cov(T_i, T_j) = \Sigma_{\chi^2}
  \]

- The sample size n is from 100 to 800
- The number of main effect is 34 (p = 34)

### correlation of $T_i$ and $T_j$

Assume $Cov(X_i,X_j) = \sigma_{ij}, ~~ Var(X_i) = \sigma_i^2$, $E(X_i) = 0$ and constant variance, then we have
\[
  Var(X_i) = E(X_i^2) - E(X_i)^2 = E(X_i^2) = \sigma_i^2 = \sigma^2
\]

\begin{align*}
  Cov(T_i, T_j) = Cov(X_i^2, X_j^2) &= E\left((X_i^2 - E(X_i^2))(X_j^2 - E(X_j^2))\right) \\ 
                                    &= E(X_i^2X_j^2 - X_i^2E(X_j^2) - X_j^2E(X_i^2) + E(X_i^2)E(X_j^2)) \\
                                    &= E(X_i^2X_j^2) - \sigma^4 \\ 
                                    &= \sigma_i^2\sigma_j^2 + 2\sigma_{ij}^2 - \sigma^4 \\ 
                                    &= 2\sigma^2_{ij}
\end{align*}

\begin{align*}
  Cor(T_i, T_j) &= \frac{Cov(X_i^2, X_j^2)}{\sqrt{Var(X_i^2)Var(X_j^2)}}\\
                &= \frac{2\sigma^2_{ij}}{2 \sigma^4} \\
                &= \frac{2(\rho\sigma^2)^2}{2 \sigma^4} \\
                &= \rho^2
\end{align*}


\newpage

### Compound Symmetry

\[
  T = [T_1 \dots, T_p] ,~~~ T_i \sim \chi_{(1)}^2, ~~~ cov(T_i, T_j) = 2\rho^2,~~~ \forall  i \ne j, \rho = \{0.1, \dots, 0.9 \} 
\]

![Compound Stymmetry](./generate_graph_as_pdf/plot_chi_fixed_fixed_total_p_34_rho_0.1_0.9_n_100_800_svd_red_0.5.pdf)


### Autoregression AR(1)

\[
  T = [T_1 \dots, T_p] ,~~~ T_i \sim \chi_{(1)}^2, ~~~ cov(T_i, T_j) = 2\rho^{2|i-j|},~~~ \forall  i \ne j, \rho = \{0.1, \dots, 0.9 \} 
\]

![AR(1)](./generate_graph_as_pdf/plot_chi_fixed_fixed_ar_chi_rho_0.1_0.9_n_100_800_p_34_svd_0.5.pdf)


### Unstructure

\[
  T = [T_1 \dots, T_p] ,~~~ T_i \sim \chi_{(1)}^2, ~~~ cov(T_i, T_j) = \sigma_{ij}
\]

![Unstructure](./generate_graph_as_pdf/plot_chi_fixed_fixed_total_un_chi_rho_0.1_0.9_n_100_800_p_34_svd_0.5.pdf)

\newpage

# PCBs data simulation result

We are using the PCBs data from the 

## sample matrix of PCB data 

```{r sample variance of PCB, echo = FALSE, comment= NA}
PCB <- read.csv("~/dev/projects/Chen_environmental_study/R_code/data/pcb_99_13_no_missing.csv")
PCB <- PCB[,-1]
cov_PCB <- cov(PCB)
cor_PCB <- cor(PCB)
```

### A glimsp of the covariance matrix 

```{r sub matrix of covariance of PCB, echo = FALSE}
cor_PCB[1:10,1:10] %>% round(., 3) %>% 
  knitr::kable(., "latex", caption = "Covariance of PCB") %>%
  kableExtra::kable_styling(latex_options = c("scale_down", "hold_position"), position = "center")
```

### Histgram of diagonal and off-diagonal elements of the PCBs'sample covariance

```{r, echo = FALSE}
par(mfrow=c(2,1))
hist(diag(cov_PCB), breaks = 30)
hist(cov_PCB[lower.tri(cov_PCB)], breaks = 30)
```

### Histgram of off-diagonal elements of the PCBs'sample correlation-coefficient

```{r, echo = FALSE}
par(mfrow=c(2,1))
hist(cor_PCB[lower.tri(cor_PCB)], breaks = 30)
```

Based on the correlation coefficient values, it seems that there is no an obvious pattern and the correlations are basically uniformly distributed. Thus, the sample covariance of PCB is more likely to have an unstructure structure. 

## Simulation result

One thing about the PCB simulation is that we are using sub-sampling to evaluate the performance of the PCB data. 

![PCB with 0.2](./generate_graph_as_pdf/plot_PCB_fixed_fixed_pro_0.1_0.9_p_33_svd_0.2.pdf)

![PCB with 0.5](./generate_graph_as_pdf/plot_PCB_fixed_fixed_pro_0.1_0.9_p_33_svd_0.5.pdf)

![PCB with 0.8](./generate_graph_as_pdf/plot_PCB_fixed_fixed_pro_0.1_0.9_p_33_svd_0.8.pdf)

