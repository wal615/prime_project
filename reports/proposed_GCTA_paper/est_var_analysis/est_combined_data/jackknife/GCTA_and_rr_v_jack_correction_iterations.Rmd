---
title: "Jackknife variance estimation corrections"
author: "Xuelong Wang"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: true
    number_sections: true
    keep_tex: true
    fig_width: 12
    fig_height: 3
header-includes:
    - \usepackage{float,amsmath, bbm, amssymb, siunitx, bm}
    - \usepackage{pdfpages}
    - \floatplacement{figure}{H}
    - \newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = "", message = FALSE, warning = FALSE, echo = FALSE, fig.height = 10)
options(scipen=1, digits=2)

library(MASS)
library(tidyverse)
library(data.table)
library(ggforce)
library(ggpubr)
library(gridExtra)
library(kableExtra)
library(knitr)
```

# Jackknife variance correction 

If we assume the $S$ is a smooth functions of emperical CDF, especially a quadratic functions, then it can be shown the leading terms of $E(\tilde{Var}(S(X_1, \dots, S_{n-1}))) \geq Var(S(X_1, \dots, S_{n-1}))$ is a quadratic term in expecation. Therefore we could try to estimate the quadratic term and correct the bias for the jackknife variance estimation.

Define $Q_{ii'} \equiv nS - (n-1)(S_{i} + S_{i'}) + (n-2)S_{(ii')}$, then the correction will be 
\[
\hat{Var}^{corr}(S(X_1, \dots, X_n)) = \hat{Var}(S(X_1, \dots, X_n)) - \frac{1}{n(n-1)}\sum_{i < i'}(Q_{ii'}- \bar{Q})^2
\]
where $\bar{Q} = \sum_{i < i'}(Q_{ii'})/(n(n-1)/2)$

# Simulation study compare two GCTA and GCTA_rr
GCTA_rr is the `mixed.solve` function from `rrBLUP` r package.  
Based on the following simulation results, 

1. when $n<p$ case, those two methods' results are very closed to each other. 
1. when $n>p$ case, in terms of effect estimation and jackknife variance estimation those two methods's reuslts are similar to each other. But for the variance corrections are quite different. That is the statistics $Q$ of our method has a very large variance which leads to negative correction result.  

### setup 
- Independent 
- Normal
- $p = 100$
- $n = \{50, 75,100, 150, 200\}$
- with interaction terms
- main effect: $Var(X^T\beta) = \{0,8,100\}$ 
\newpage

```{r, include=F}
source("~/dev/projects/Chen_environmental_study/reports/proposed_GCTA_paper/est_var_analysis/est_combined_data/jackknife/GCTA_and_rr_v_jack_multiple_iteration.R")
```

### Simulation result 
### $Var(X^T\beta) = \{0\}$ 
```{r, }
summary_final_GCTA_0
summary_final_GCTA_rr_0
```

### $Var(X^T\beta) = \{100\}$ 
```{r, }
summary_final_GCTA_100
summary_final_GCTA_rr_100
```

### $Var(X^T\beta) = \{8\}$ 
```{r, }
summary_final_GCTA_8
summary_result_GCTA_500
summary_final_GCTA_rr_8
summary_result_GCTA_rr_500

```

### correlation test $ 
```{r, }
summary_final_cor_500
```

## compare the performance of delete 1 and delete d in variance estimation 
The delete-d jackknife varinace estimator is 
\[
\mathcal{v}_{J(d)} = \frac{n-d}{d} \cdot \frac{1}{S}\sum_{S}(\hat{\theta}_s - \hat{\theta}_{s.} )
\],
where $S =\binom{n}{d}$. 
Note that S could a very large value, so in the following simulation, only $S = 1000$ is used. In Jun Shao's another paper, he proposed an approximation of the deletel-d variance estimation. That is just select m from $S =\binom{n}{d}$ sub-samples and in that paper it recommended $m = n^{1.5}$.

### setup 
- Independent 
- Normal
- $p = \{100, 1000\}$
- $n = \{50, 75,100, 150, 200, 500, 750,1000, 1500, 2000\}$
- $d = 0.5 \times n$
- $n_{repeat} = 1000$ for delete d jackknife 
- main effect: $Var(X^T\beta) = 8$ 


\newpage

```{r, include = F}
source("~/dev/projects/Chen_environmental_study/reports/proposed_GCTA_paper/est_var_analysis/est_combined_data/jackknife/delete_d_GCTA_and_rr_v_jack_multiple_iteration.R")
```

### GCTA with p = 100 
```{r, echo = F}
rbind(summary_final_GCTA_8_d_1, summary_final_GCTA_8_d_d, summary_final_GCTA_8_d_d_m, fill = TRUE) %>% 
  kable(., "latex", longtable = T, booktabs = T) %>%
  kable_styling(latex_options = c("repeat_header"), font_size = 7)
```

### GCTA with p = 1000 
```{r, echo = F}
rbind(summary_final_GCTA_8_d_1_1000, summary_final_GCTA_8_d_d_1000) %>% 
  kable(., "latex", longtable = T, booktabs = T) %>%
  kable_styling(latex_options = c("repeat_header"), font_size = 7)
```

### GCTA_rr_rr with p = 100 
```{r, echo = F}
rbind(summary_final_GCTA_rr_8_d_1, summary_final_GCTA_rr_8_d_d, summary_final_GCTA_rr_8_d_d_m, fill = TRUE) %>% 
  kable(., "latex", longtable = T, booktabs = T) %>%
  kable_styling(latex_options = c("repeat_header"), font_size = 7)
```

### GCTA_rr with p = 1000 
```{r, echo = F}
rbind(summary_final_GCTA_rr_8_d_1_1000, summary_final_GCTA_rr_8_d_d_1000) %>% 
  kable(., "latex", longtable = T, booktabs = T) %>%
  kable_styling(latex_options = c("repeat_header"), font_size = 7)
```

### cor with n = 200
```{r, echo = F}
options(scipen=1, digits=5)
rbind(summary_final_cor_8_d_1_100, summary_final_cor_8_d_d_100) %>% 
  kable(., "latex", longtable = T, booktabs = T) %>%
  kable_styling(latex_options = c("repeat_header"), font_size = 7)
```

### median with n = 200
```{r, echo = F}
options(scipen=1, digits=5)
rbind(summary_final_median_8_d_1_1000, summary_final_median_8_d_d_1000) %>% 
  kable(., "latex", longtable = T, booktabs = T) %>%
  kable_styling(latex_options = c("repeat_header"), font_size = 7)
```

## Jackknife variance estimation's bias and sample size n 

### setup 
- Independent 
- Normal
- $p = 100$
- $n = \{50, 75,100, 150, 200, 500, 750,1000, 1500, 2000\}$
- $d = 0.5 \times n$
- $n_{repeat} = n^{1.5}$ for delete d jackknife 
- main effect: $Var(X^T\beta) = 8$ 


Based on the previous simulation results, we find there is a bias among all the jackknife variance estimation. Based on the Efron's result, the overestimation is because the statistics $S$ is not a smooth function of the distribution function, so that the correct coefficient actually inflate the variance estimation. 

The following reuslt is trying to see the relation between the bias and the sample size n 

```{r, include = F}
source("~/dev/projects/Chen_environmental_study/reports/proposed_GCTA_paper/est_var_analysis/est_combined_data/jackknife/delete_d_1_EG_GCTA_rr_for_v_jack_plot.R")
```

### GCTA with p = 100 
```{r, echo = F}
data <- rbind(summary_final_GCTA_rr_8_d_1_50_2000,
              summary_final_GCTA_rr_8_d_d_50_2000,
              fill = TRUE) %>% setorder(., by = n)

kable(data, "latex", longtable = T, booktabs = T) %>%
kable_styling(latex_options = c("repeat_header"), font_size = 5)
data[, d:= ifelse(d == 1, "1", "d")]
ggplot(data = data, 
       aes(x=n, y=relative_ratio, group=d)) +
  geom_line(aes(color=d)) +
  geom_point(aes(color=d)) +
  ylim(c(0,2)) +
  theme_bw()
```

### Eg with p = 100 
```{r, echo = F}
data <- rbind(summary_final_EigenPrism_8_d_1_50_2000,
              summary_final_EigenPrism_8_d_d_50_2000,
              fill = TRUE) %>% setorder(., by = n)

kable(data, "latex", longtable = T, booktabs = T) %>%
kable_styling(latex_options = c("repeat_header"), font_size = 5)
data[, d:= ifelse(d == 1, "1", "d")]
ggplot(data = data, 
       aes(x=n, y=relative_ratio, group=d)) +
  geom_line(aes(color=d)) +
  geom_point(aes(color=d)) +
  ylim(c(0,2)) +
  theme_bw()
```


