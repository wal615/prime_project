---
title: "Decorrelation methods and their effects on proposed method"
author: "Xuelong Wang"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: true
    number_sections: true
    keep_tex: true
    fig_width: 12
    fig_height: 4.5
header-includes:
    - \usepackage{float,amsmath, bbm, siunitx, bm}
    - \floatplacement{figure}{H}
    - \newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = FALSE, message = FALSE, warning = FALSE, fig.height = 10)
library(MASS)
library(tidyverse)
source("../../R_code/Yang_REML.R")
```

# Motivation
Based on the previous simulation result, we found that the decorrelation step has a big influence on the final performance of the proposed method. More specifically, when the $n<p$ is happening then we known that the sample covariance matrix $\Sigma_{X}$ is not full rank. Therefore, $\Sigma^{-1}_X$, the inverse of  $\Sigma_{X}$, doesn't exist. So we could calculate the general inverse of the covariance matrix $\Sigma_{X}$. In such situation, I just adapted one of commonly used g-inverse -- the Moore penrose inverse $\Sigma^{+}_X$ during the decorrelation procedure. But the result is not very well compared with the original method. Thus, the following is trying to discuss the reason of why this is not working. 

# SVD decorrelation procedure

\[
  Var(X) = \Sigma_X = U\Lambda V^T,
\]

- $X$ is the random vector with dim as $p \times 1$,  
- $\Sigma_X$ is $p \times p$ symmetry matrix,  
- $U = V$ are orthogonal matrix and each column is the eigenvector  
- $\Lambda$ is a diagonal matrix with each diagonal element as the eigenvalue.   

## Assume the $\Sigma_X$ is full rank
To decorreate the X, we could just take the reciprocal of each square root of eigenvalue as following. 

\[
  \Sigma^{-\frac{1}{2}}_X = U\Lambda^{-\frac{1}{2}}U^T,
\]
where $\Lambda^{-\frac{1}{2}} = \begin{bmatrix}
                        e_1^{-\frac{1}{2}} & \dots & 0 \\
                        \vdots & \ddots & \vdots \\
                        0 &  \dots  & e_p^{-\frac{1}{2}}
                        \end{bmatrix}$

So that after transformation the $\Sigma^{-\frac{1}{2}}_X X$ has identity covariance matrix as following,

\[
  Var(\Sigma^{-\frac{1}{2}}_X X) = \Sigma^{-\frac{1}{2}}_X \Sigma_X\Sigma^{-\frac{1}{2}}_X = U\Lambda^{-\frac{1}{2}}U^T U\Lambda^{-1}U^T U\Lambda^{-\frac{1}{2}}U^T = I_p.
\]

## Assume the $\Sigma_X$ is not full rank

\[
  Var(X) = \Sigma_X = U\Lambda V^T =
                        \begin{bmatrix}
                         U_1 & U_2\\
                        \end{bmatrix}
                        \begin{bmatrix}
                        \Lambda_1 & 0\\
                        0 & 0
                        \end{bmatrix}
                        \begin{bmatrix}
                        U_1^T \\
                        U_2^T
                        \end{bmatrix} = U_1\Lambda_1U_1^T,
\]
- $U_1$ is a $p \times r$ matrix with r < p and in most of case r = n the sample size.

Then after applying the same procedure we get following, 

\[
  \Sigma^{-\frac{1}{2}}_X = U_1\Lambda_1^{-\frac{1}{2}}U_1^T,
\]
Note that in this case, I'm using Moore Penrose inverse. 

After transformation the X we have, 
\[
  Var(\Sigma^{-\frac{1}{2}}_X X) = \Sigma^{-\frac{1}{2}}_X \Sigma_X\Sigma^{-\frac{1}{2}}_X = U_1\Lambda^{-\frac{1}{2}}_1U^T_1 U_1\Lambda^{-1}_1U^T_1 U_1\Lambda^{-\frac{1}{2}}_1U^T_1 = U_1U_1^T,  
\]
Note that by the property of the U we have 
\[
  U_1U_1^T + U_2U_2^T = I_p, ~~~~~\\
  (U_1U_1^T)^T U_1U_1^T = U_1U_1^T,
\]
Besides, $U_1U_1^T$ and $U_2U_2^T$ are indempotent and $rank(U_2U_2^T) + rank(U_1U_1^T) = p$.  

So if the X is not full rank we cannot decorrelation the covariance matrix to an identity matrix.

## Simulation stduy  

### Simulation 1

```{r, collapse=TRUE, tidy=TRUE, echo=TRUE}
# How the singular sample covariance affect the SVD decorrelation result
set.seed(123)
p <- 200
n <- 200
Sig <- matrix(rep(0.5, 200*200), ncol = 200)
diag(Sig) <- 1
x_total <- mvrnorm(n, numeric(p), Sigma = Sig)

x_100 <- x_total[1:100,]
Est_sqrt_ins_cov_100 <- invsqrt(cov(x_100))
cor(x_100%*%Est_sqrt_ins_cov_100) [1:5, 1:5] %>% round(.,4)
cor(x_100%*%Est_sqrt_ins_cov_100) %>% abs(.) %>% sum(.)
cov(x_100%*%Est_sqrt_ins_cov_100) %>% diag(.) %>% sum(.)
cor(x_100%*%Est_sqrt_ins_cov_100)[cor(x_100%*%Est_sqrt_ins_cov_100) %>% 
                                    lower.tri(., diag = FALSE)] %>% max()

x_200 <- x_total
Est_sqrt_ins_cov_200 <- invsqrt(cov(x_200))
cor(x_200%*%Est_sqrt_ins_cov_200)[1:5,1:5] %>% round(.,4)
cor(x_200%*%Est_sqrt_ins_cov_200) %>% abs(.) %>% sum(.)
cov(x_200%*%Est_sqrt_ins_cov_200) %>% diag(.) %>% sum(.)
cor(x_200%*%Est_sqrt_ins_cov_200)[cor(x_200%*%Est_sqrt_ins_cov_200) %>% 
                                    lower.tri(., diag = FALSE)] %>% max()

# if we use the inverse information of x_200
cor(x_100%*%Est_sqrt_ins_cov_200)[1:5,1:5] %>% round(.,4)
cor(x_100%*%Est_sqrt_ins_cov_200) %>% abs(.) %>% sum(.)
cov(x_100%*%Est_sqrt_ins_cov_200) %>% diag(.) %>% sum(.)
cor(x_100%*%Est_sqrt_ins_cov_200)[cor(x_100%*%Est_sqrt_ins_cov_200) %>% 
                                    lower.tri(., diag = FALSE)] %>% max()
```

- As we expected, when $n < p$, SVD decorrelation's result is not as good as full rank case ( $n\geq p$), which means off diagonal elements are not equal or closed to zero
- The largest correlation coefficient of $X_{100}$ is 0.28 and the for $X_{200}$ is 0.0036
- If we use the sample variance of $X_{200}$ to decorrelate $X_{100}$, it seems there is a little improvement on the max correlation coefficient

### Simulation 2

Next, I tried to evaluate the performance of the GCTA and proposed method under the $n < p$ scenario. To keep the problem simple, I just let the covariates to be independent to each other. We want to see if the original GCTA method is affected by the singular sample covariance matrix problem. If it was able to work fine, then it verify the proposed method. More specifically, If we could find a linear transformation on covariates X to make them independent or un-correlated at least in population level (although we still have the singular sample covariance matrix problem), then we could still get a unbiased estimation of the total covariates variance.

#### A toy sample 
```{r collapse=TRUE, tidy=TRUE, echo = TRUE}
x <- mvrnorm(100, numeric(200), diag(200))
cor(x) %>% abs(.) %>% sum(.)
max(cor(x)[lower.tri(cor(x))])
```

- this is to show that although the covariates are independent in population level, their sample correlation may still be large
- the GCTA method may be effected by the this problem or not 

#### Simulation set up

\[
  X = [x_1^2, \dots, x_p^2]^T
\]

- $p = 34$  
- $x_i \stackrel{iid}{\sim} N(0,1)$  
- So $x_i^2$ are iid chi-square with $df = 1$  

We consider to estimate the total variance, so that the total covariates (combined main and interaction terms) is 595. Let the sample size n increase from 100 to 600 in order to see how the $p < n$ simulation affect the GCTA and proposed method's performance. Following is the simulation result.

```{r, include=FALSE}
source("./generate_graph_chi.R")
```

#### Simulation results

```{r fixed_fixed, fig.cap = "Fixed_Fixed total independet chi with df = 1"}
plot_chi_fixed_fixed_total_p_34_n_100_600
```


```{r fixed_random, fig.cap = "fixed_random total independet chi with df = 1"}
plot_chi_fixed_random_total_p_34_n_100_600
```


```{r random_random, fig.cap = "random_random total independet chi with df = 1"}
plot_chi_random_random_total_p_34_n_100_600
```

- The original GCTA method is not affected by the $p < n$ condition if the covariates are independent in the population level
- note that actual the covariates are not perfectly independent because of the interaction terms
- the proposed method is affected a lot by the $p <n$ situation, but it getting better when sample size is increasing

# Lasso regression decorrelation procedure

## Motivation 
Based on the previous result, we find that the original GCTA is not sensitive to the $p < n$ situation. So the next question is to find a way to decorrelating the covariates so that they become uncorrelated in the population level. The SVD method seems not work well in that situation as previous simulations shows. Therefore, we look for the lasso regression method. 

## Main Idea and step

The main idea of lasso regression is to find a procedure of decorrelation. we could consider each covaraite as the dependent variable and select several other covariates as the independent variables. Then, we could preformance a lasso regression and use the residual as the new covariates. After doing that the residuals should be uncorrelated to each other. 

\[
  X = [X_1, \dots X_p]
\]

- X is a $n \times p$ observed covariates matrix
- $X_i, ~~ i = 1, \dots, p$ are the columns of X

1. For each $X_i$, select a group $\mathcal{A}_t$ of variable as the independent variable
1. Conduct lasso regression and decide the final group of active variables as the independent variables $\mathcal{A}_f$, note that $\mathcal{A}_f \subset \mathcal{A}_t$
1. Conduct a linear regression with $Y = X_i ~~ X = X_j, j\in\mathcal{A}_f$ and use the residual as the new covariate $Z_i = X_i - \hat{X_i}$

# Sigularity of the sample covariance matrix

Based on the simulation study, the results suggest that the proposed method has a bias in total (ultimate) effect estimation when $n<p$ situation. One of the most relative reason is that the way we used to estimate the sample covariance matrix. We suspect that the singularity of the sample covariance (Note its $rank \leq p-1$, because it using sample mean to calculate the sample covariance) affects the result the proposed method. 

## Solution 1, using SVD to reduce the total covariate matrix into a $n \times n$ matrix

### Main idea 

\[
  X = U \Sigma V^T = \begin{bmatrix}
                      U_1 & U_2
                      \end{bmatrix}
                      \begin{bmatrix}
                      \Lambda_1 & 0\\
                      0 & 0
                      \end{bmatrix}
                      \begin{bmatrix}
                      V_1 & V_2\\
                      V_3 & V_4
                      \end{bmatrix}^T 
                      = 
                      \begin{bmatrix}
                      U_1\Lambda_1 & 0
                      \end{bmatrix}
                      \begin{bmatrix}
                      V_1^T & V_3^T\\
                      V_2^T & V_4^T
                      \end{bmatrix}
                      =
                      \begin{bmatrix}
                      U_1\Lambda_1V_1^T & U_1\Lambda_1V_3^T
                      \end{bmatrix}
\]
Ignore the $V_3$ , then we have the X_r as following
\[
  X_r = U_1\Lambda_1V_1^T.
\]
We use $X_r$ as the new covariates to the proposed methd. Therefore, we reduce the dimension from p to n. 

### Simulation setup 

\[
  X = [X_1 \dots, X_p] ,~~~ X_i \sim \chi_{(1)}^2, ~~~ cov(X_i, X_j) = 0.25,~~~ \forall  i \ne j 
\]


- The sample size n is from 100 to 700
- The number of main effect is 34 (p = 34)
- The number of interaction effect is 561
- The number of totoal effect is 595

Therefore, when $n\leq591$, we using the svd_dimension reduction method. \textcolor{red}{When $n > 595$, we don't reduce the dimension of the total covariates}



### Simulation result 

```{r load picture svd, include=FALSE }
source("./generate_graph_chi.R")
```


```{r fixed_fixed_svd_sim_red, fig.cap = "Fixed_Fixed total corr = 0.25 chi with df = 1"}
plot_chi_fixed_fixed_total_p_34_n_100_700_svd_dim_red
```


```{r fixed_random_sim_red, fig.cap = "fixed_random total corr = 0.25 chi with df = 1"}
plot_chi_fixed_random_total_p_34_n_100_700_svd_dim_red
```


```{r random_random_sim_red, fig.cap = "random_random total corr = 0.25 chi with df = 1"}
plot_chi_random_random_total_p_34_n_100_700_svd_dim_red
```