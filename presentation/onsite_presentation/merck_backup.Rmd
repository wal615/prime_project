---
title: "Representative approach for \n big data dimension reduction with binary responses"
short-title: "Representative approach"
author: "Xuelong Wang and Jie Yang"
date: '`r format(Sys.Date(), "%B %d, %Y")`'      # Month DD, YYYY (Main Slide)
short-date: '`r format(Sys.Date(), "%m/%d/%Y")`' # MM/DD/YYYY (Lower Right)
institute: "University of Illinois at Chicago"
short-institute: "UIC"
department: "Department of Mathematics, Computer Science, and Statistics"  
section-titles: false        # Provides slide headings
safe-columns: true   # Enables special latex macros for columns.
bibliography: bibliography.bib
header-includes:
   - \usepackage{amsmath, bbm, graphicx,multirow}
   - \usepackage{booktabs}
   - \usepackage{longtable}
   - \usepackage{array}
   - \usepackage{multirow}
   - \usepackage{wrapfig}
   - \usepackage{float}
   - \usepackage{colortbl}
   - \usepackage{pdflscape}
   - \usepackage{tabu}
   - \usepackage{threeparttable}
   - \usepackage{threeparttablex}
   - \usepackage[normalem]{ulem}
   - \usepackage{makecell}
   - \usepackage{xcolor}
   - \newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}
output: 
   uiucthemes::beamer_illinois: 
      toc: true
---

### Backup

\begin{examples}

1. Linear regression: $Y = a + b_1^TX + b_2^TX + \epsilon$

2. NonLinear regression: $Y = a + \exp(b_1^TX) + \sin(b_2^TX) + \epsilon$

3. More general: $Y = f(b_1^TX, b_2^TX, \epsilon)$
\end{examples}

### Subspace

- Vector space U: $\vec{\mathbf{a}}, \vec{\mathbf{b}} \in U$ 
    1. $\vec{\mathbf{a}} + \vec{\mathbf{b}} \in U$   
    2. $\lambda \vec{\mathbf{a}} \in U, \lambda \in \mathbb{R}$  
    
- Subspace $V$: Given k independent vectors $(\vec{\mathbf{a}}_1, \dots, \vec{\mathbf{a}}_k), ~~\vec{\mathbf{a}}_i \in \mathbb{R}^p$, 
\[
V = \mathcal{L}((\vec{\mathbf{a}}_1, \dots, \vec{\mathbf{a}}_k) = \{\sum_{i = 1}^k\lambda_ia_i, \lambda_i\in \mathbb{R}\}
\]
$V$ is spaced by $(\vec{\mathbf{a}}_1, \dots, \vec{\mathbf{a}}_k)$

- A basis of $V$: $(\vec{\mathbf{a}}_1, \dots, \vec{\mathbf{a}}_k)$ is called a basis of $V$, but it is not unique

### SIR
1. $E(X|Y) - E(X)$ is p-dimensional curves as Y varies and lies in a k-dimensional subspace
1. The covariance matrix of $E(X|Y) - E(X)$ is degenerate at any direction that orthogonal to $\Sigma_{X}b_i,i = 1, \dots, d$
1. Candidate Matrix:  $M_{SIR} = Var(E(X|Y) - E(X)) = Var(E(X|Y))$
1. $S_{SIR} := Span(\Sigma_{X}^{-1}M_{SIR}) \subseteq S_{Y|X}$
1. $\Sigma_{X}^{-1}M_{SIR}b_i = \lambda_i b_i$ $b_i$ is the ith eigenvector of $\Sigma_{X}^{-1}M_{SIR}$ 



```{r, echo = FALSE}
dir_list <- readRDS("~/dev/projects/presentation/onsite_presentation/est_dir")
```
### Simulation estimated direction: Proposed SAVE
```{r, echo = FALSE, comment="", fig.cap="rep SAVE"}
dir_list$rep_save %>% round(.,2) 
```

### Simulation estimated direction: Proposed SIR
```{r, echo = FALSE, comment="", fig.cap="rep SAVE"}
dir_list$rep_sir %>% round(.,2) 
```

### Simulation estimated direction: PRE SIR
```{r, echo = FALSE, comment="", fig.cap="rep SAVE"}
dir_list$PRE_sir %>% round(.,2) 
```

### Real Data analysis 

#### SUSY data
- $n = 5 \times 10^7$
- $X$ are 18 features of a physics experiment of particles in high-energy
- $Y$ is binary 

#### How evalute the estimated directions
- Don't know the true central space so no distance measure
- Classification performance but depends on the classification model



<!-- \begin{table}[] -->
<!-- \centering -->
<!-- \caption{Simulation result of SIR  \newline  -->
<!--          Significant level 0.05\newline  -->
<!--          directions of central subsapce $d =3$} -->
<!-- \resizebox{\textwidth}{!}{% -->
<!-- \begin{tabular}{|c|c|cccc|cccc|cccc|} -->
<!-- \hline -->
<!-- \multicolumn{2}{|l|}{\multirow{2}{*}{}}              & \multicolumn{4}{c|}{Original SIR} & \multicolumn{4}{c|}{PRE SIR} & \multicolumn{4}{c|}{Proposed SIR}                 \\ \cline{3-14} -->
<!-- \multicolumn{2}{|l|}{}                               & \multicolumn{12}{c|}{log n}                                                                                    \\ \hline -->
<!-- \multicolumn{1}{|l|}{}    & Direction/Distance       & 3      & 4      & 5      & 6     & 3        & 4    & 5    & 6    & 3    & 4          & 5          & 6          \\ -->
<!-- \multirow{3}{*}{Power}    & 0D vs \textgreater{}= 1D & 1      & 1      & 1      & 1     & 1        & .    & .    & .    & 0.75 & \color{blue}\textbf{1} & \color{blue}\textbf{1} & \color{blue}\textbf{1} \\ -->
<!--                           & 1D vs \textgreater{}= 2D & .      & .      & .      & .     & 1        & .    & .    & .    & 0.16 & \color{blue}\textbf{1} & \color{blue}\textbf{1} & \color{blue}\textbf{1} \\ -->
<!--                           & 2D vs \textgreater{}= 3D & .      & .      & .      & .     & 1     & .    & .    & .    & 0.01 & 0.01       & 0          & 0.01       \\ \hline -->
<!-- \multirow{3}{*}{Type-I}   & 3D vs \textgreater{}= 4D & .      & .      & .      & .     & 0      & .    & .    & .    & 0    & 0          & 0          & 0          \\ -->
<!--                           & 4D vs \textgreater{}= 5D & .      & .      & .      & .     & 0      & .    & .    & .    & 0    & 0          & 0          & 0          \\ -->
<!--                           & 5D vs \textgreater{}= 6D & .      & .      & .      & .     & 0    & .    & .    & .    & 0    & 0          & 0          & 0          \\ \hline -->
<!-- \multirow{2}{*}{Distance} & F                        & 1.14   & 1.12   & 1.14   & 1.13  & \color{red}\textbf{0.88}     & .    & .    & .    & 1.47 & 1.13       & \color{blue}\textbf{1.01}       & \color{blue}\textbf{1}       \\ -->
<!--                           & R                        & 0.01   & 0   & 0   & 0  & \color{blue}\textbf{0.06}     & .    & .    & .    & 0.06 & 0.02  & \color{blue}\textbf{0}       & \color{blue}\textbf{0}       \\ \hline -->
<!-- \end{tabular}% -->
<!-- } -->
<!-- \end{table} -->
